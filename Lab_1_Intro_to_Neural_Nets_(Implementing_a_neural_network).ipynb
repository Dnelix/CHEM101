{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dnelix/CHEM101/blob/master/Lab_1_Intro_to_Neural_Nets_(Implementing_a_neural_network).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this lab we will implement a simple neural network.\n",
        "![](https://res.cloudinary.com/ogbanugot/image/upload/v1706913740/full_network_bias_e33llw.jpg)    \n"
      ],
      "metadata": {
        "id": "iuSdB5Ok33TH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First let's implement the neural network in pure Python."
      ],
      "metadata": {
        "id": "h9pjrAgA4z3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382Kj7S_8T4P"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Define the sigmoid and relu activation function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return max(0, x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input vector\n",
        "x = [0.6, 0.4, 0.5, 0.9]\n",
        "\n",
        "\n",
        "# Define the hidden  weight matrix\n",
        "w_h = [[0.1, -0.3, 0.56, -0.22],\n",
        "       [0.12, 0.2, -0.31, -0.4],\n",
        "       [-0.5, 0.16, 0.36, -0.56],\n",
        "       [0.62, -0.57, 0.43, -0.71]]\n",
        "\n",
        "# Define the output weight matrix\n",
        "w_o = [[0.48, -0.51, 0.28, -0.36],\n",
        "       [0.11, -0.71, -0.37, -0.41]]\n",
        "\n",
        "# Define the bias for the hidden layer\n",
        "b_h = [0.1, 0.1, 0.1, 0.1]\n",
        "\n",
        "# Define the bias for the output layer\n",
        "b_o = [0.2, 0.2]\n",
        "\n",
        "# Define the number of output neurons\n",
        "num_outputs = 2"
      ],
      "metadata": {
        "id": "8G7LJwd58op3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the hidden layer outputs\n",
        "hidden_outputs = []\n",
        "for j in range(len(w_h)):\n",
        "    net_input = sum(w_h[j][i] * x[i] for i in range(len(x))) + b_h[j]\n",
        "    hidden_outputs.append(round(net_input, 2))\n",
        "\n",
        "# Print the hidden layer outputs before applying activations\n",
        "print(\"Hidden Layer Outputs Before Activation:\")\n",
        "for k in range(len(hidden_outputs)):\n",
        "    print(f\"Neuron {k + 1}: {hidden_outputs[k]}\")"
      ],
      "metadata": {
        "id": "z2J-LL_18rDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cf8f56-4d9f-49ea-bd63-4231f6e9cea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Layer Outputs Before Activation:\n",
            "Neuron 1: 0.12\n",
            "Neuron 2: -0.26\n",
            "Neuron 3: -0.46\n",
            "Neuron 4: -0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply relu activation function to the hidden layer outputs\n",
        "hidden_activations = [relu(output) for output in hidden_outputs]\n",
        "\n",
        "# Print the hidden layer activations\n",
        "print(\"\\nHidden Layer Activations:\")\n",
        "for k in range(len(hidden_activations)):\n",
        "    print(f\"Neuron {k + 1}: {hidden_activations[k]}\")"
      ],
      "metadata": {
        "id": "nX5T6QoJ8to4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67937472-6c23-4d65-ffde-31056ea5c360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden Layer Activations:\n",
            "Neuron 1: 0.12\n",
            "Neuron 2: 0\n",
            "Neuron 3: 0\n",
            "Neuron 4: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the output layer outputs\n",
        "output_outputs = []\n",
        "for j in range(len(w_o)):\n",
        "    net_input = sum(w_o[j][i] * hidden_activations[i] for i in range(len(hidden_activations))) + b_o[j]\n",
        "    output_outputs.append(round(net_input, 2))\n",
        "\n",
        "# Print the output layer outputs before applying activations\n",
        "print(\"\\nOutput Layer Outputs Before Activation:\")\n",
        "for k in range(num_outputs):\n",
        "    print(f\"Neuron {k + 1}: {output_outputs[k]}\")"
      ],
      "metadata": {
        "id": "JJNOBJah8wAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf29d1b-44d6-4306-e7f7-b38c62028cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output Layer Outputs Before Activation:\n",
            "Neuron 1: 0.26\n",
            "Neuron 2: 0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sigmoid activation function to the output layer outputs\n",
        "output_activations = [round(sigmoid(output), 2) for output in output_outputs]\n",
        "\n",
        "# Print the output layer activations\n",
        "print(\"\\nOutput Layer Activations:\")\n",
        "for k in range(num_outputs):\n",
        "    print(f\"Neuron {k + 1}: {output_activations[k]}\")"
      ],
      "metadata": {
        "id": "cDOc_IQQ8yU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8806ef-96a1-4046-e15c-78d09018088e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output Layer Activations:\n",
            "Neuron 1: 0.56\n",
            "Neuron 2: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next let's use Numpy arrays and operations to implement the neural network."
      ],
      "metadata": {
        "id": "RGIabzOd5KqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid and relu activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define the input vector\n",
        "x = np.array([0.6, 0.4, 0.5, 0.9])\n",
        "\n",
        "# Define the hidden weight matrix\n",
        "w_h = np.array([[0.1, -0.3, 0.56, -0.22],\n",
        "                [0.12, 0.2, -0.31, -0.4],\n",
        "                [-0.5, 0.16, 0.36, -0.56],\n",
        "                [0.62, -0.57, 0.43, -0.71]])\n",
        "\n",
        "# Define the output weight matrix\n",
        "w_o = np.array([[0.48, -0.51, 0.28, -0.36],\n",
        "                [0.11, -0.71, -0.37, -0.41]])\n",
        "\n",
        "# Define the bias for the hidden layer\n",
        "b_h = np.array([0.1, 0.1, 0.1, 0.1])\n",
        "\n",
        "# Define the bias for the output layer\n",
        "b_o = np.array([0.2, 0.2])"
      ],
      "metadata": {
        "id": "CAE6QySm241O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the hidden layer outputs\n",
        "hidden_outputs = np.dot(w_h, x) + b_h\n",
        "\n",
        "# Print the hidden layer outputs before applying activations\n",
        "print(\"Hidden Layer Outputs Before Activation:\")\n",
        "print(hidden_outputs)"
      ],
      "metadata": {
        "id": "rbwhveKu3NGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60b6dc5-db4f-4102-8bc4-34447abd7f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Layer Outputs Before Activation:\n",
            "[ 0.122 -0.263 -0.46  -0.18 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply relu activation function to the hidden layer outputs\n",
        "hidden_activations = relu(hidden_outputs)\n",
        "\n",
        "# Print the hidden layer activations\n",
        "print(\"\\nHidden Layer Activations:\")\n",
        "print(hidden_activations)"
      ],
      "metadata": {
        "id": "LddnCw4U3Qj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b5cfc0-03e0-4660-c60c-258da1c610f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden Layer Activations:\n",
            "[0.122 0.    0.    0.   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the output layer outputs\n",
        "output_outputs = np.dot(w_o, hidden_activations) + b_o\n",
        "\n",
        "# Print the output layer outputs before applying activations\n",
        "print(\"\\nOutput Layer Outputs Before Activation:\")\n",
        "print(output_outputs)"
      ],
      "metadata": {
        "id": "cZxBC72l3VBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481bb625-aa47-438d-c68f-46bf78b45c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output Layer Outputs Before Activation:\n",
            "[0.25856 0.21342]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sigmoid activation function to the output layer outputs\n",
        "output_activations = sigmoid(output_outputs)\n",
        "\n",
        "# Print the output layer activations\n",
        "print(\"\\nOutput Layer Activations:\")\n",
        "print(output_activations)"
      ],
      "metadata": {
        "id": "PDxGJrhY3ZM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8e6aa9-2b0a-447f-bc49-21be803a91af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output Layer Activations:\n",
            "[0.56428228 0.5531534 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lab Challenge"
      ],
      "metadata": {
        "id": "sSNGOeN8JCCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the following neural network using numpy.\n",
        "![](https://res.cloudinary.com/ogbanugot/image/upload/v1707495405/test_network_alehxr.png)  \n"
      ],
      "metadata": {
        "id": "BPOUA8mR5euo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill in the code sections below."
      ],
      "metadata": {
        "id": "7gA0MPxhodL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid and relu activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Define the input vector\n",
        "x = np.array([0.2, 0.9, 0.6])\n",
        "\n",
        "# Define the hidden weight matrix\n",
        "w_h = np.array([[-0.09, -0.18, 0.06],\n",
        "                [0.5, -0.99, 0.12]])\n",
        "\n",
        "# Define the output weight vector\n",
        "w_o = np.array([-0.89, 0.23])\n",
        "\n",
        "# Define the bias for the hidden layer\n",
        "b_h = np.array([0.4, 0.4])\n",
        "\n",
        "# Define the bias for the output layer\n",
        "b_o = np.array([0.1])"
      ],
      "metadata": {
        "id": "qDdlhIBrnkJF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the hidden layer outputs\n",
        "hidden_outputs = np.dot(w_h, x) + b_h\n",
        "\n",
        "# Print the hidden layer outputs before applying activations\n",
        "print(\"Hidden Layer Outputs Before Activation:\")\n",
        "print(hidden_outputs)"
      ],
      "metadata": {
        "id": "aEEau5KyoCD0",
        "outputId": "124e3a22-cbf1-48c5-f6fe-8120e22f63b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Layer Outputs Before Activation:\n",
            "[ 0.256 -0.319]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply relu activation function to the hidden layer outputs\n",
        "hidden_activations = relu(hidden_outputs)\n",
        "\n",
        "# Print the hidden layer activations\n",
        "print(\"\\nHidden Layer Activations:\")\n",
        "print(hidden_activations)"
      ],
      "metadata": {
        "id": "v_sVeCwzoCD0",
        "outputId": "e8a6b401-5941-4e58-d999-0b81bfe3395d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden Layer Activations:\n",
            "[0.256 0.   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the output layer outputs\n",
        "output_outputs = np.dot(w_o, hidden_activations) + b_o\n",
        "\n",
        "# Print the output layer outputs before applying activations\n",
        "print(\"\\nOutput Layer Outputs Before Activation:\")\n",
        "print(output_outputs)"
      ],
      "metadata": {
        "id": "WzTnjJ7ooCD0",
        "outputId": "6d3f45f0-3579-4cf8-eaaa-3c793c2a65b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output Layer Outputs Before Activation:\n",
            "[-0.12784]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sigmoid activation function to the output layer outputs\n",
        "output_activations = sigmoid(output_outputs)\n",
        "\n",
        "# Print the output layer activations\n",
        "print(\"\\nOutput Layer Activations:\")\n",
        "print(output_activations)"
      ],
      "metadata": {
        "id": "Cq86esyooCD1",
        "outputId": "4f1e989b-1fb1-40a1-8f9f-fb3a10488373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output Layer Activations:\n",
            "[0.46808346]\n"
          ]
        }
      ]
    }
  ]
}